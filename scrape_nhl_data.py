# -*- coding: utf-8 -*-
"""Scrape NHL data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D8hax2Bs-k-8HVwttJVLnlEqAzNhWOEM

Hello friend,

üìä üèí Accessing data is the first challenge of any analysis, and in lots of cases, it can be a tedious task. For some, how difficult it is can even stop their ambition to do the hockey project they want to work on. I have been there too.

This is why I made a notebook that includes functions to scrape data from NHL.com for people who might be interested in hockey data. You can use those functions and adapt them for your own needs.

I hope I can help some of you!


---
If you enjoyed this notebook, follow me on [Twitter](https://x.com/woumaxx) and [Bluesky](https://bsky.app/profile/maxtixador.bsky.social)

First, let's import the libraries we will use to scrape the data.
"""

# Install Selenium
!pip install selenium

# Install Chrome
!apt-get update
!apt-get install -y chromium-chromedriver

import pandas as pd
import numpy as np
import requests
import json
from bs4 import BeautifulSoup

# Set environment path to ChromeDriver
import os
os.environ["PATH"] += ":/usr/lib/chromium-browser/"


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

import time

from functools import lru_cache
from tqdm import tqdm


# import matplotlib.pyplot as plt
# import seaborn as sns

pd.set_option('display.max_columns', None)

# pd.set_option('display.max_rows', None)

"""#Draft

## Selections
"""

def scrapeDraft(year = 2023, round = "all"):
    """
    Scrapes draft data from the NHL website for a given year and round.

    Parameters :
      - year (int) : The year of the Draft you want to scrape the data from. Default is set to 2023.
      - round (int/str) : The round of the Draft you want to scrape the data from. It generally takes integers, but you can have "all" to get all rounds. Default is set to "all".

    Returns :
      - draft_df (pd.DataFrame) : A DataFrame containing the scraped draft data.

    """


    url = f"https://api-web.nhle.com/v1/draft/picks/{year}/{round}"

    response = requests.get(url).json()

    draft_df = pd.json_normalize(response["picks"])

    # Add meta data (datetime of the execution)
    draft_df["meta_datetime"] = pd.to_datetime("now")



    return draft_df


scrapeDraft()

"""## Central of Scouting Rankings"""

def scrapeRankings(year=2025, category=1):
    """
    Scrapes draft rankings from the NHL website for a given year and category.

    Parameters :
      - year (int) : The year of the Draft you want to scrape the data from. Default is set to 2024.
      - category (int) : The category of the Draft you want to scrape the data from. Default is set to 1.
        1 = North American Skaters
        2 = International Skaters
        3 = North American Goalies
        4 = International Goalies

    Returns :
      - draft_rankings_df (pd.DataFrame) : A DataFrame containing the scraped draft rankings data.

    """

    # By the Central of Scouting of the NHL

    url = f"https://api-web.nhle.com/v1/draft/rankings/{year}/{category}"

    # Which category is what id
    categoryDict = {
        "north-american-skater": 1,
        "international-skater": 2,
        "north-american-goalie": 3,
        "international-goalie": 4
    }



    response = requests.get(url).json()

    draft_rankings_df = pd.json_normalize(response["rankings"])

    # Add meta data (datetime of the execution)
    draft_rankings_df["meta_datetime"] = pd.to_datetime("now")

    return draft_rankings_df


scrapeRankings()

"""# Stats - Work in progress"""

# Stats on NHL.com
url_template = f"https://api.nhle.com/stats/rest/en/skater/summary?isAggregate={{{'is_aggregate'}}}&isGame={{{'is_game'}}}&sort={{{'sort'}}}&start={{{'start'}}}&limit={{{'limit'}}}&cayenneExp=gameTypeId={{{'game_type_id'}}}%20and%20seasonId%3C={{{'season_id_max'}}}%20and%20seasonId%3E={{{'season_id_min'}}}"

url = url_template.format(
    is_aggregate="false",
    is_game="false",
    sort="%5B%7B%22property%22:%22pointsPerGame%22,%22direction%22:%22DESC%22%7D,%7B%22property%22:%22goals%22,%22direction%22:%22DESC%22%7D,%7B%22property%22:%22assists%22,%22direction%22:%22DESC%22%7D,%7B%22property%22:%22playerId%22,%22direction%22:%22ASC%22%7D%5D",
    start="0",
    limit="50",
    game_type_id="2",
    season_id_max="20242025",
    season_id_min="20242025"
)

# "https://api.nhle.com/stats/rest/en/skater/summary?isAggregate=false&isGame=false&sort=%5B%7B%22property%22:%22pointsPerGame%22,%22direction%22:%22DESC%22%7D,%7B%22property%22:%22playerId%22,%22direction%22:%22ASC%22%7D%5D&start=0&limit=50&factCayenneExp=gamesPlayed%3E=30&cayenneExp=gameTypeId=2%20and%20seasonId%3C=20242025%20and%20seasonId%3E=20242025"


url = "https://api.nhle.com/stats/rest/en/skater/summary?isAggregate=false&isGame=false&sort=%5B%7B%22property%22:%22points%22,%22direction%22:%22DESC%22%7D,%7B%22property%22:%22goals%22,%22direction%22:%22DESC%22%7D,%7B%22property%22:%22assists%22,%22direction%22:%22DESC%22%7D,%7B%22property%22:%22playerId%22,%22direction%22:%22ASC%22%7D%5D&start=0&limit=1000&cayenneExp=franchiseId%3D1%20and%20gameTypeId=2%20and%20seasonId%3C=20242025%20and%20seasonId%3E=20232024"
response = requests.get(url).json()

stats_df = pd.json_normalize(response["data"])


stats_df.head()

### IGNORE THIS LOL

(
    stats_df
    .query("teamAbbrevs == 'MTL'")
    .pivot_table(index=["playerId", "skaterFullName", "positionCode"], columns=["seasonId"], values=["timeOnIcePerGame", "gamesPlayed"], aggfunc="sum")
    .sort_values(by=[("timeOnIcePerGame", 20242025)], ascending=False)
    .dropna()
    # .head(10)
    .assign(
        combined = lambda x: x[("timeOnIcePerGame", 20232024)] * x[("gamesPlayed", 20232024)]/  (x[("gamesPlayed", 20232024)] + x[("gamesPlayed", 20242025)] )+ x[("timeOnIcePerGame", 20242025)]*  x[("gamesPlayed", 20242025)]/(x[("gamesPlayed", 20232024)] + x[("gamesPlayed", 20242025)] ), #weighed average
        TOI_diff=lambda x: (x["combined"] - x[("timeOnIcePerGame", 20232024)]) / 60)
        # TOI_diff=lambda x: (x[("timeOnIcePerGame", 20242025)] - x[("timeOnIcePerGame", 20232024)]) / 60)
    .sort_values(by="TOI_diff", ascending=False)
    .reset_index()  # Reset index for plotting
    .plot(
        x="skaterFullName",
        y="TOI_diff",
        kind="bar",
        legend=False,
        title="Time on Ice Difference Between 2023-2024 and 2023-2025 Seasons (in minutes)",
        xlabel="Player",
        ylabel="TOI Difference (minutes)"
    ).invert_xaxis()
)

"""#Teams"""

def scrapeTeams():
    """
    Scrapes team data from the NHL website. Generally to get team IDs for future use.

    Parameters :
      - None

    Returns :
      - teams_df (pd.DataFrame) : A DataFrame containing the scraped team data.

    *** It does not contain team abbreviations

    """


    url = "https://api.nhle.com/stats/rest/en/franchise?sort=fullName&include=lastSeason.id&include=firstSeason.id"

    response = requests.get(url).json()

    teams_df = pd.json_normalize(response["data"])
    teams_df = teams_df.rename(columns={"id": "teamId"})

    # Add meta data (datetime of the execution)
    teams_df["meta_datetime"] = pd.to_datetime("now")

    return teams_df

scrapeTeams().head()

"""# Schedule"""

def scrapeSchedule(team_slug, season):
    """
    Scrapes schedule data from the NHL website for a given team and season.

    Parameters :
      - team_slug (str) : The slug of the team you want to scrape the schedule data for.
      - season (str/int) : The season you want to scrape the schedule data for in the format of "YYYYYYYY".

    Returns :
      - schedule_df (pd.DataFrame) : A DataFrame containing the scraped schedule data.

    """

    url = f"https://api-web.nhle.com/v1/club-schedule-season/{team_slug}/{season}"

    response = requests.get(url).json()

    schedule_df = pd.json_normalize(response["games"])
    schedule_df["teamAbbrev"] = team_slug

    # Add meta data (datetime of the execution)
    schedule_df["meta_datetime"] = pd.to_datetime("now")

    return schedule_df

scrapeSchedule("MTL", "20242025").head()

"""# Game

## Play-by-Play
"""

def scrapePlayByPlay(gameId):

    """
    Scrapes play-by-play data from the NHL website for a given game ID.

    Parameters :
      - game_id (int) : The ID of the game you want to scrape the play-by-play data for.

      Returns :
      - pbp_df (pd.DataFrame) : A DataFrame containing the scraped play-by-play data.

    """



    url = f"https://api-web.nhle.com/v1/gamecenter/{gameId}/play-by-play"

    response = requests.get(url).json()

    pbp_df = pd.json_normalize(response["plays"])


    pbp_df["gameId"] = gameId


    # Add meta data (datetime of the execution)
    pbp_df["meta_datetime"] = pd.to_datetime("now")

    return pbp_df

pbp_df = scrapePlayByPlay(2024020445)

pbp_df.head().columns

@lru_cache(maxsize=1000)  # Cache up to 1000 unique gameIds
def scrape_pbp(gameId):
    """
    Scrapes play-by-play data from the NHL website for a given game ID.

    Parameters :
      - game_id (int) : The ID of the game you want to scrape the play-by-play data for.

      Returns :
      - pbp_df (pd.DataFrame) : A DataFrame containing the scraped play-by-play data.
      - rosters_df (pd.DataFrame) : A DataFrame containing the scraped roster data.
      - shifts_df (pd.DataFrame) : A DataFrame containing the scraped shift data.


    """



    url = f"https://api-web.nhle.com/v1/gamecenter/{gameId}/play-by-play"

    response = requests.get(url).json()

    pbp_df = pd.json_normalize(response["plays"])

    rosters_df = pd.json_normalize(response['rosterSpots'])
    rosters_df['fullName'] = rosters_df['firstName.default'] + ' ' + rosters_df['lastName.default']

    rosters_dict = dict(zip(rosters_df['playerId'], rosters_df['fullName']))

    shifts_url = f"https://api.nhle.com/stats/rest/en/shiftcharts?cayenneExp=gameId={gameId}"

    shifts_response = requests.get(shifts_url).json()

    shifts_df = pd.json_normalize(shifts_response["data"])
    shifts_df['startTime_s'] = shifts_df['startTime'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1])) + (shifts_df['period'] -1)* 20 * 60
    shifts_df['endTime_s'] = shifts_df['endTime'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1])) + (shifts_df['period'] -1)* 20 * 60
    shifts_df['fullName'] = shifts_df['firstName'] + ' ' + shifts_df['lastName']

    shifts_df['side'] = np.where(shifts_df['teamAbbrev'] == response['homeTeam']['abbrev'], 'home', 'away')


    # Compare columns
    og_cols = ['eventId', 'timeInPeriod', 'timeRemaining', 'situationCode',
       'homeTeamDefendingSide', 'typeCode', 'typeDescKey', 'sortOrder',
       'periodDescriptor.number', 'periodDescriptor.periodType',
       'periodDescriptor.maxRegulationPeriods', 'details.eventOwnerTeamId',
       'details.losingPlayerId', 'details.winningPlayerId', 'details.xCoord',
       'details.yCoord', 'details.zoneCode', 'details.hittingPlayerId',
       'details.hitteePlayerId', 'details.blockingPlayerId',
       'details.shootingPlayerId', 'details.reason', 'details.shotType',
       'details.goalieInNetId', 'details.awaySOG', 'details.homeSOG',
       'details.playerId', 'details.typeCode', 'details.descKey',
       'details.duration', 'details.committedByPlayerId',
       'details.drawnByPlayerId', 'pptReplayUrl', 'details.scoringPlayerId',
       'details.scoringPlayerTotal', 'details.assist1PlayerId',
       'details.assist1PlayerTotal', 'details.assist2PlayerId',
       'details.assist2PlayerTotal', 'details.awayScore', 'details.homeScore',
       'details.highlightClipSharingUrl', 'details.highlightClipSharingUrlFr',
       'details.highlightClip', 'details.highlightClipFr',
       'details.discreteClip', 'details.discreteClipFr',
       'details.secondaryReason', 'details.servedByPlayerId', 'zoneStartSide_1','zoneStartSideDetail_1']
    pbp_columns = set(pbp_df.columns.tolist())
    expected_columns = set(og_cols)

    if pbp_columns != expected_columns:
        new_columns = pbp_columns - expected_columns
        missing_columns = expected_columns - pbp_columns
        if new_columns:
            print(f"New columns in the dataset: {new_columns}")

        if missing_columns:
            # print(f"Missing columns from the dataset: {missing_columns}. We will create them.")
            for col in missing_columns:
                pbp_df[col] = np.nan

        # raise ValueError("Column mismatch detected. See details above.")

    # Life's always better when you have team abbreviation in the dataframe instead of just the teamId
    abbrev_dict = {
        response['awayTeam']['id'] : response['awayTeam']['abbrev'],
        response['homeTeam']['id'] : response['homeTeam']['abbrev']
    }
    pbp_df['eventTeam'] = pbp_df['details.eventOwnerTeamId'].map(abbrev_dict)
    pbp_df["gameId"] = gameId

    pbp_df['period'] = pd.to_numeric(pbp_df['periodDescriptor.number'])

    # Test to see if events we are not aware of
    expected_events = ['period-start', 'faceoff', 'hit', 'blocked-shot', 'shot-on-goal',
       'stoppage', 'giveaway', 'delayed-penalty', 'penalty',
       'missed-shot', 'goal', 'takeaway', 'period-end',
       'shootout-complete', 'game-end', ]

    actual_events = pbp_df['typeDescKey'].unique()

    missing_events = set(actual_events) - set(expected_events)

    if missing_events:
        raise ValueError(f"The following events are not in the dataset: {missing_events}")

    # Rename cols
    # pbp_df

    # # Make playerId_1, playerId_2, playerId_3
    # pbp_df['playerId_1'] = np.nan
    # pbp_df['playerName_1'] = np.nan

    # pbp_df['playerId_2'] = np.nan
    # pbp_df['playerName_2'] = np.nan

    # pbp_df['playerId_3'] = np.nan
    # pbp_df['playerName_3'] = np.nan

    # ## if faceoff
    # pbp_df.loc[pbp_df["typeDescKey"] == 'faceoff','playerId_1'] = pbp_df.loc[pbp_df["typeDescKey"] == 'faceoff','details.winningPlayerId']
    # pbp_df.loc[pbp_df["typeDescKey"] == 'faceoff','playerId_2'] = pbp_df.loc[pbp_df["typeDescKey"] == 'faceoff','details.losingPlayerId']

    # pbp_df.loc[pbp_df["typeDescKey"] == 'hit','playerId_1'] = pbp_df.loc[pbp_df["typeDescKey"] == 'hit','details.hittingPlayerId']
    # pbp_df.loc[pbp_df["typeDescKey"] == 'hit','playerId_2'] = pbp_df.loc[pbp_df["typeDescKey"] == 'hit','details.hitteePlayerId']

    # pbp_df.loc[pbp_df["typeDescKey"] == 'blocked-shot','playerId_1'] = pbp_df.loc[pbp_df["typeDescKey"] == 'blocked-shot','details.shootingPlayerId']
    # pbp_df.loc[pbp_df["typeDescKey"] == 'blocked-shot','playerId_2'] = pbp_df.loc[pbp_df["typeDescKey"] == 'blocked-shot','details.blockingPlayerId']

    # pbp_df.loc[pbp_df["typeDescKey"].isin(['shot-on-goal', 'missed-shot']) ,'playerId_1'] = pbp_df.loc[pbp_df["typeDescKey"].isin(['shot-on-goal', 'missed-shot']),'details.shootingPlayerId']

    # pbp_df.loc[pbp_df["typeDescKey"] == 'goal' ,'playerId_1'] = pbp_df.loc[pbp_df["typeDescKey"] == 'goal','details.scoringPlayerId']
    # pbp_df.loc[pbp_df["typeDescKey"] == 'goal' ,'playerId_2'] = pbp_df.loc[pbp_df["typeDescKey"] == 'goal','details.assist1PlayerId']
    # pbp_df.loc[pbp_df["typeDescKey"] == 'goal' ,'playerId_3'] = pbp_df.loc[pbp_df["typeDescKey"] == 'goal','details.assist2PlayerId']

    # pbp_df.loc[pbp_df["typeDescKey"].isin(['giveaway', 'takeaway']),'playerId_1'] = pbp_df.loc[pbp_df["typeDescKey"].isin(['giveaway', 'takeaway']),'details.playerId']

    # pbp_df.loc[pbp_df["typeDescKey"] == 'penalty','playerId_1'] = pbp_df.loc[pbp_df["typeDescKey"] == 'penalty','details.committedByPlayerId']
    # pbp_df.loc[pbp_df["typeDescKey"] == 'penalty','playerId_2'] = pbp_df.loc[pbp_df["typeDescKey"] == 'penalty','details.drawnByPlayerId']
    # # pbp_df.loc[pbp_df["typeDescKey"] == 'penalty','playerId_3'] = pbp_df.loc[pbp_df["typeDescKey"] == 'penalty','details.servedByPlayerId']

    # Player mapping and event assignments
    event_columns = {
        'faceoff': ('details.winningPlayerId', 'details.losingPlayerId'),
        'hit': ('details.hittingPlayerId', 'details.hitteePlayerId'),
        'blocked-shot': ('details.shootingPlayerId', 'details.blockingPlayerId'),
        'shot-on-goal': ('details.shootingPlayerId', None),
        'missed-shot': ('details.shootingPlayerId', None),
        'goal': ('details.scoringPlayerId', 'details.assist1PlayerId', 'details.assist2PlayerId'),
        'giveaway': ('details.playerId', None),
        'takeaway': ('details.playerId', None),
        'penalty': ('details.committedByPlayerId', 'details.drawnByPlayerId', 'details.servedByPlayerId')
    }

    # Initialize player columns
    pbp_df[['playerId_1', 'playerId_2', 'playerId_3']] = np.nan
    pbp_df[['playerName_1', 'playerName_2', 'playerName_3']] = np.nan

    # Assign player data based on event type
    for event, columns in event_columns.items():
        for i, col in enumerate(columns, start=1):
            if col:
                pbp_df.loc[pbp_df['typeDescKey'] == event, f'playerId_{i}'] = pbp_df.loc[pbp_df['typeDescKey'] == event, col]


    for col in ['details.awaySOG', 'details.homeSOG', 'details.awayScore', 'details.homeScore'] :
        pbp_df[col] = pbp_df[col].ffill().fillna(0)



    # Dynamically find columns that contain the word 'Clip'
    clip_columns = pbp_df.filter(like='Clip').columns.tolist()


    # Define the columns to drop, including dynamically found clip columns
    columns_to_drop = [
        # 'details.eventOwnerTeamId',
        'details.losingPlayerId', 'details.winningPlayerId',
        'details.hittingPlayerId', 'details.hitteePlayerId', 'details.shootingPlayerId',
        'details.blockingPlayerId', 'details.playerId', 'details.committedByPlayerId',
        'details.drawnByPlayerId', 'periodDescriptor.maxRegulationPeriods', 'situationCode',
        'typeCode', 'pptReplayUrl',
        'details.scoringPlayerId', 'details.assist1PlayerId', 'details.assist2PlayerId',
        'details.servedByPlayerId'

    ] + clip_columns

    # Drop the specified columns
    pbp_df = pbp_df.drop(columns=columns_to_drop)

    pbp_df['playerName_1'] = pbp_df['playerId_1'].map(rosters_dict)
    pbp_df['playerName_2'] = pbp_df['playerId_2'].map(rosters_dict)
    pbp_df['playerName_3'] = pbp_df['playerId_3'].map(rosters_dict)

    pbp_df = pbp_df.rename(columns={
        'typeDescKey' : 'event',
        'periodDescriptor.number' : 'periodNumber',
        'periodDescriptor.periodType' : 'periodType',
        'details.eventOwnerTeamId' : 'teamId',
        'details.xCoord' : 'xCoord',
        'details.yCoord' : 'yCoord',
        'details.zoneCode' : 'zoneCode',
        'details.reason' : 'reason',
        'details.shotType' : 'shotType',
        'details.goalieInNetId' : 'goalieInNetId',
        'details.awaySOG' : 'awaySOG',
        'details.homeSOG' : 'homeSOG',
        'details.typeCode' :	'typeCode',
        'details.descKey' : 'descKey',
        'details.duration' : 'duration',
        'details.scoringPlayerTotal' : 'scoringPlayerTotal',
        'details.assist1PlayerTotal' : 'assist1PlayerTotal',
        'details.assist2PlayerTotal' : 'assist2PlayerTotal',
        'details.awayScore' : 'awayScore',
        'details.homeScore' : 'homeScore',
        'details.secondaryReason' : 'secondaryReason'


        })

    # Elapsed time in seconds
    pbp_df['timeInPeriod_s'] = pbp_df['timeInPeriod'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
    pbp_df['timeRemaining_s'] = pbp_df['timeRemaining'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
    pbp_df['elapsedTime'] = (pbp_df['period']- 1) * 20 * 60 + pbp_df['timeInPeriod_s']


    pbp_df['homeTeam'] = response['homeTeam']['abbrev']
    pbp_df['awayTeam'] = response['awayTeam']['abbrev']

    pbp_df['homeTeamId'] = response['homeTeam']['id']
    pbp_df['awayTeamId'] = response['awayTeam']['id']

    pbp_df['eventTeamType'] = np.where(pbp_df['eventTeam'] == pbp_df['homeTeam'], 'home', 'away')

    pbp_df['xFixed'] = np.where(
    ((pbp_df['eventTeamType'] == "home") & (pbp_df['homeTeamDefendingSide'] == "right")) |
    ((pbp_df['eventTeamType'] == "away") & (pbp_df['homeTeamDefendingSide'] == "right")),
    0 - pbp_df['xCoord'],
    pbp_df['xCoord']
    )

    pbp_df['yFixed'] = np.where(
        ((pbp_df['eventTeamType'] == "home") & (pbp_df['homeTeamDefendingSide'] == "right")) |
        ((pbp_df['eventTeamType'] == "away") & (pbp_df['homeTeamDefendingSide'] == "right")),
        0 - pbp_df['yCoord'],
        pbp_df['yCoord']
    )

    stoppages_times = pbp_df.query("event == 'faceoff'")['elapsedTime'].to_list()

    shifts_df['type_on'] = np.where(
        shifts_df['startTime_s'].isin(stoppages_times),
        'SIP', # Stoppage-in-Play
        'OTF' # On-the-Fly
    )
    shifts_df['type_off'] = np.where(
        shifts_df['endTime_s'].isin(stoppages_times),
        'SIP', # Stoppage-in-Play
        'OTF' # On-the-Fly
    )

    # faceoff_dots = {
    #     "center_ice": {
    #         "description": "The center ice faceoff dot, located at the middle of the rink.",
    #         "coordinates": (0, 0),
    #     },
    #     "neutral_zone_offside": {
    #         "description": "The four faceoff dots near the blue lines in the neutral zone.",
    #         "dots": [
    #             {"location": "Right Blue Line, Left Side", "coordinates": (25, 22)},
    #             {"location": "Right Blue Line, Right Side", "coordinates": (25, -22)},
    #             {"location": "Left Blue Line, Left Side", "coordinates": (-25, 22)},
    #             {"location": "Left Blue Line, Right Side", "coordinates": (-25, -22)},
    #         ],
    #     },
    #     "offensive_zone": {
    #         "description": "The two main faceoff dots in the offensive zone.",
    #         "dots": [
    #             {"location": "Left Offensive Dot", "coordinates": (69, 22)},
    #             {"location": "Right Offensive Dot", "coordinates": (69, -22)},
    #         ],
    #     },
    #     "defensive_zone": {
    #         "description": "The two main faceoff dots in the defensive zone, mirrored from the offensive zone.",
    #         "dots": [
    #             {"location": "Left Defensive Dot", "coordinates": (-69, 22)},
    #             {"location": "Right Defensive Dot", "coordinates": (-69, -22)},
    #         ],
    #     },
    #     "low_zone": {
    #         "description": "Faceoff dots near the goal crease, used for plays closer to the goal.",
    #         "dots": [
    #             {"location": "Low Dot, Right Offensive Zone", "coordinates": (69, 5)},
    #             {"location": "Low Dot, Left Offensive Zone", "coordinates": (69, -5)},
    #             {"location": "Low Dot, Right Defensive Zone", "coordinates": (-69, 5)},
    #             {"location": "Low Dot, Left Defensive Zone", "coordinates": (-69, -5)},
    #         ],
    #     },
    # }

    # Create the initial data for the faceoff dots
    fac = {
    'name': [
        'Center Ice', 'Neutral Zone Right Blue Line Left Side',
        'Neutral Zone Right Blue Line Right Side',
        'Neutral Zone Left Blue Line Left Side',
        'Neutral Zone Left Blue Line Right Side',
        'Offensive Zone Left Dot', 'Offensive Zone Right Dot',
        'Defensive Zone Left Dot', 'Defensive Zone Right Dot',
        'Low Zone Offensive Right Dot', 'Low Zone Offensive Left Dot',
        'Low Zone Defensive Right Dot', 'Low Zone Defensive Left Dot'
    ],
    'x_coord': [0, 20, 20, -20, -20, 69, 69, -69, -69, 69, 69, -69, -69],
    'y_coord': [0, 22, -22, 22, -22, 22, -22, 22, -22, 5, -5, 5, -5],
    'home_team_side': [
        'Centre', 'Neutral Zone', 'Neutral Zone', 'Neutral Zone', 'Neutral Zone',
        'Offensive Zone', 'Offensive Zone', 'Defensive Zone', 'Defensive Zone',
        'Offensive Zone', 'Offensive Zone', 'Defensive Zone', 'Defensive Zone'
    ],
    'away_team_side': [
        'Centre', 'Neutral Zone', 'Neutral Zone', 'Neutral Zone', 'Neutral Zone',
        'Defensive Zone', 'Defensive Zone', 'Offensive Zone', 'Offensive Zone',
        'Defensive Zone', 'Defensive Zone', 'Offensive Zone', 'Offensive Zone'
    ],
    'home_team_side_detail': [
        'Centre', 'Neutral Offensive Zone Right',
        'Neutral Offensive Zone Left', 'Neutral Defensive Zone Right',
        'Neutral Defensive Zone Left', 'Offensive Zone Right',
        'Offensive Zone Left', 'Defensive Zone Right',
        'Defensive Zone Left', 'Offensive Zone Right',
        'Offensive Zone Left', 'Defensive Zone Right', 'Defensive Zone Left'
    ],
    'away_team_side_detail': [
        'Centre', 'Neutral Defensive Zone Left',
        'Neutral Defensive Zone Right', 'Neutral Offensive Zone Left',
        'Neutral Offensive Zone Right', 'Defensive Zone Left',
        'Defensive Zone Right', 'Offensive Zone Left',
        'Offensive Zone Right', 'Defensive Zone Left',
        'Defensive Zone Right', 'Offensive Zone Left', 'Offensive Zone Right'
    ]
    }
    # Create the DataFrame directly from the dictionary
    faceoff_df = pd.DataFrame(fac)
    faceoff_df['xFixed'] = faceoff_df['x_coord']
    faceoff_df['yFixed'] = faceoff_df['y_coord']

    pbp_df = pbp_df.merge(faceoff_df.drop(columns=['name', 'x_coord', 'y_coord']),
                          on=['xFixed', 'yFixed'],how='left')


    # Where was the faceoff taken (if player has a SIP Start)
    shifts_df = shifts_df.merge(
        pbp_df.query("event == 'faceoff'")[['elapsedTime', 'home_team_side', 'away_team_side',
                                            'home_team_side_detail', 'away_team_side_detail']],
        left_on='startTime_s',
        right_on='elapsedTime',
        how='left'
    )

    shifts_df["zoneStartSide"] = np.where(
        shifts_df["side"] == "home",
        shifts_df["home_team_side"],
        shifts_df["away_team_side"])

    shifts_df["zoneStartSideDetail"] = np.where(
        shifts_df["side"] == "home",
        shifts_df["home_team_side_detail"],
        shifts_df["away_team_side_detail"])

    shifts_df = shifts_df.drop(columns=['elapsedTime', 'home_team_side', 'away_team_side',
                                        'home_team_side_detail', 'away_team_side_detail'])
    shifts_df['is_home'] = np.where(shifts_df['side'] == 'home', 1, 0)


    rosters_df.loc[rosters_df['positionCode'].isin(["L", "C", "R", "W"]), 'position'] = 'F'
    rosters_df.loc[rosters_df['positionCode'].isin(["D"]), 'position'] = 'D'
    rosters_df.loc[rosters_df['positionCode'].isin(["G"]), 'position'] = 'G'
    shifts_df = shifts_df.merge(
        rosters_df[['playerId', 'position']],
        on='playerId',
        how='left'
    )

    rosters_df['is_home'] = (rosters_df['teamId'] == response.get('homeTeam', {}).get('id', "")).astype(int)
    # Prepare filtered DataFrames
    home_skaters_df = shifts_df.query("position != 'G' and is_home == 1")
    away_skaters_df = shifts_df.query("position != 'G' and is_home == 0")
    home_goalies_df = shifts_df.query("position == 'G' and is_home == 1")
    away_goalies_df = shifts_df.query("position == 'G' and is_home == 0")

    # Precompute elapsed times
    elapsed_times = pbp_df['elapsedTime']

    # Determine skater IDs and goalie IDs by elapsed time
    home_sktrs_id = [
        home_skaters_df.loc[(home_skaters_df['startTime_s'] <= second) & (home_skaters_df['endTime_s'] > second), 'playerId'].unique().tolist()
        for second in elapsed_times
    ]

    away_sktrs_id = [
        away_skaters_df.loc[(away_skaters_df['startTime_s'] <= second) & (away_skaters_df['endTime_s'] > second), 'playerId'].unique().tolist()
        for second in elapsed_times
    ]

    home_goalie_id = [
        home_goalies_df.loc[(home_goalies_df['startTime_s'] <= second) & (home_goalies_df['endTime_s'] > second), 'playerId'].unique()
        for second in elapsed_times
    ]
    home_goalie_id = [ids[0] if len(ids) == 1 else np.nan for ids in home_goalie_id]

    away_goalie_id = [
        away_goalies_df.loc[(away_goalies_df['startTime_s'] <= second) & (away_goalies_df['endTime_s'] > second), 'playerId'].unique()
        for second in elapsed_times
    ]
    away_goalie_id = [ids[0] if len(ids) == 1 else np.nan for ids in away_goalie_id]

    # Count skaters
    n_home_sktrs = [len(ids) for ids in home_sktrs_id]
    n_away_sktrs = [len(ids) for ids in away_sktrs_id]

    # Assign skater counts and goalie IDs
    pbp_df['home_skaters'] = n_home_sktrs
    pbp_df['away_skaters'] = n_away_sktrs
    pbp_df['home_goalie_id'] = home_goalie_id
    pbp_df['away_goalie_id'] = away_goalie_id

    # Determine game strength
    pbp_df['is_home'] = pbp_df['eventTeamType'].map({'home': 1, 'away': 0})
    pbp_df['game_strength'] = pbp_df.apply(
        lambda row: f"{row['home_skaters']}v{row['away_skaters']}" if row['is_home'] == 1 else f"{row['away_skaters']}v{row['home_skaters']}",
        axis=1
    )

    # Determine max number of skaters on ice
    max_column_index = max(len(ids) for ids in home_sktrs_id + away_sktrs_id)

    # Prepare column names
    columns_to_add = [f"home_skater_id{j+1}" for j in range(max_column_index)] + \
                    [f"away_skater_id{j+1}" for j in range(max_column_index)] + \
                    [f"home_skater_fullName{j+1}" for j in range(max_column_index)] + \
                    [f"away_skater_fullName{j+1}" for j in range(max_column_index)]

    # Initialize columns with proper type (object)
    for column in columns_to_add:
        pbp_df[column] = ""

    id_name_dict = rosters_df.set_index('playerId')['fullName'].to_dict()
    # Assign values to the DataFrame for skater IDs and full names
    # Assign skater IDs and names
    for i, (home_skater_ids, away_skater_ids) in enumerate(zip(home_sktrs_id, away_sktrs_id)):
        for j, skater_id in enumerate(home_skater_ids):
            pbp_df.at[i, f"home_skater_id{j+1}"] = skater_id
            pbp_df.at[i, f"home_skater_fullName{j+1}"] = id_name_dict.get(skater_id, "")
        for j, skater_id in enumerate(away_skater_ids):
            pbp_df.at[i, f"away_skater_id{j+1}"] = skater_id
            pbp_df.at[i, f"away_skater_fullName{j+1}"] = id_name_dict.get(skater_id, "")


    # pbp_df = pbp_df.replace('NaN', np.nan, inplace=False)

    pbp_df['home_goalie_id'] = home_goalie_id
    pbp_df['away_goalie_id'] = away_goalie_id

    pbp_df.loc[(pbp_df['goalieInNetId'].notnull()) & (pbp_df['home_goalie_id'].isna()) & (pbp_df['away_goalie_id'].isna()) & (pbp_df['is_home']== 1), "away_goalie_id"] = pbp_df['goalieInNetId']
    pbp_df.loc[(pbp_df['goalieInNetId'].notnull()) & (pbp_df['home_goalie_id'].isna()) & (pbp_df['away_goalie_id'].isna()) & (pbp_df['is_home']== 0), "home_goalie_id"] = pbp_df['goalieInNetId']



    pbp_df['home_goalie_fullName'] = pbp_df['home_goalie_id'].map(id_name_dict)
    pbp_df['away_goalie_fullName'] = pbp_df['away_goalie_id'].map(id_name_dict)


    # Add shifts to pbp
    reshaped_df = pd.melt(
    shifts_df,
    id_vars=['id', 'playerId','teamAbbrev','teamId', 'fullName', 'zoneStartSide', 'zoneStartSideDetail', 'is_home', 'type_on', 'type_off'],
    value_vars=['startTime_s', 'endTime_s'],
    var_name='event_type',
    value_name='event_time_s'
    )

    reshaped_df['typeCode'] = reshaped_df['event_type'].map({'startTime_s': 'ON', 'endTime_s': 'OFF'})


    reshaped_df['type'] = reshaped_df.apply(
        lambda row: row['type_on'] if row['typeCode'] == 'ON' else row['type_off'], axis=1
    )


    reshaped_df = reshaped_df.drop(columns=['event_type', 'type_on', 'type_off'])


    reshaped_df = reshaped_df.sort_values(['playerId', 'event_time_s', 'typeCode']).reset_index(drop=True)
    reshaped_df['event'] = 'line-change'
    reshaped_df = reshaped_df.rename(columns={'event_time_s': 'time_s',
                                          'playerId':'playerId_1',
                                          'fullName':'playerName_1',
                                          'zoneStartSide':'zoneStartSide_1',
                                          'zoneStartSideDetail':'zoneStartSideDetail_1',
                                          'event_time_s' : 'elapsedTime',
                                          'teamAbbrev' : 'eventTeam',
                                          'type' : 'descKey',
                                          'id' : 'eventId'

                                          })


    for col in pbp_df.columns.tolist():
        if col not in reshaped_df.columns.tolist():
            reshaped_df[col] = np.nan

    pbp_df = pd.concat([pbp_df, reshaped_df], ignore_index=True)

    pbp_df['eventTeamType'] = np.where(pbp_df['eventTeam'] == pbp_df['homeTeam'], 'home', 'away')
    pbp_df['eventTeamId'] = np.where(pbp_df['eventTeam'] == pbp_df['homeTeam'], pbp_df['homeTeamId'], pbp_df['awayTeamId'])

    event_priority = {
    'goal': 1,
    'penalty': 2,
    'delayed-penalty': 3,
    'shot-on-goal': 4,
    'missed-shot': 5,
    'blocked-shot': 6,
    'hit': 7,
    'takeaway': 8,
    'giveaway': 9,
    'stoppage': 10,
    'line-change': 11,
    'period-start': 12,
    'period-end': 13,
    'game-end': 14,
    'faceoff': 15
    }

    pbp_df['priority'] = pbp_df['event'].map(event_priority)
    pbp_df= pbp_df.sort_values(by=['elapsedTime', 'priority']).reset_index(drop=True)

    pbp_df['season'] = response['season']
    pbp_df['gameType'] = response['gameType']
    pbp_df['limitedScoring'] = response['limitedScoring']
    pbp_df['gameDate'] = pd.to_datetime(response['gameDate']).strftime('%Y-%m-%d')
    pbp_df['venue'] = response['venue']['default']
    pbp_df['venueLocation'] = response['venueLocation']['default']
    pbp_df['startTimeUTC'] = pd.to_datetime(response['startTimeUTC']).strftime('%Y-%m-%d %H:%M:%S')
    pbp_df['easternUTCOffset'] = response['easternUTCOffset']
    pbp_df['venueUTCOffset'] = response['venueUTCOffset']
    # pbp_df['tvBroadcasts'] = response['tvBroadcasts']
    pbp_df['gameState'] = response['gameState']
    pbp_df['gameScheduleState'] = response['gameScheduleState']
    pbp_df['gameOutcome'] = response['gameOutcome']

    # Bbfill columns
    pbp_df[['homeTeam', 'awayTeam', 'homeTeamId', 'awayTeamId', 'timeRemaining_s',
            'awayScore', 'homeScore', 'awaySOG', 'homeSOG', 'sortOrder', 'periodNumber',
            'timeInPeriod', 'timeRemaining', 'homeTeamDefendingSide']] =  pbp_df[['homeTeam', 'awayTeam', 'homeTeamId', 'awayTeamId', 'timeRemaining_s',
                                                                                  'awayScore', 'homeScore', 'awaySOG', 'homeSOG', 'sortOrder', 'periodNumber',
                                                                                  'timeInPeriod', 'timeRemaining', 'homeTeamDefendingSide']].bfill(axis ='rows')


    # Add meta data (datetime of the execution)
    current_time = pd.to_datetime("now")
    pbp_df['meta_datetime'] = current_time
    shifts_df['meta_datetime'] = current_time
    rosters_df['meta_datetime'] = current_time

    return pbp_df, rosters_df, shifts_df

pbp_df = scrape_pbp(2024020447)[0]

pbp_df.head()

# pbp_df['typeDescKey'].unique()
# ['period-start', 'faceoff', 'hit', 'blocked-shot', 'shot-on-goal',
#        'stoppage', 'giveaway', 'delayed-penalty', 'penalty',
#        'missed-shot', 'goal', 'takeaway', 'period-end',
#        'shootout-complete', 'game-end']
# pbp_df.query("typeDescKey == 'takeaway'")

pbp_df[pbp_df['event'].isin(['faceoff'])].head()

shifts_df = scrapeShifts(2024020445)
shifts_df['start'] = (shifts_df['period']- 1) * 20 * 60 + shifts_df['startTime'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
shifts_df['end'] = (shifts_df['period']- 1) * 20 * 60 + shifts_df['endTime'].str.split(':').apply(lambda x: int(x[0]) * 60 + int(x[1]))
shifts_df['duration_s'] = shifts_df['end'] - shifts_df['start']
shifts_df.head()

"""## Rosters"""

def scrapeRosters(gameId):

    """
    Scrapes roster data from the NHL website for a given game ID.

    Parameters :
      - game_id (int) : The ID of the game you want to scrape the roster data for.

      Returns :
      - rosters_df (pd.DataFrame) : A DataFrame containing the scraped roster data.

    """



    url = f"https://api-web.nhle.com/v1/gamecenter/{gameId}/play-by-play"

    response = requests.get(url).json()

    rosters_df = pd.json_normalize(response['rosterSpots'])

    # Life's always better when you have team abbreviation in the dataframe instead of just the teamId
    abbrev_dict = {
        response['awayTeam']['id'] : response['awayTeam']['abbrev'],
        response['homeTeam']['id'] : response['homeTeam']['abbrev']
    }
    rosters_df['teamAbbrev'] = rosters_df['teamId'].map(abbrev_dict)


    rosters_df["gameId"] = gameId

    # Add meta data (datetime of the execution)
    rosters_df["meta_datetime"] = pd.to_datetime("now")


    return rosters_df

rosters_df = scrapeRosters(2024020170)
rosters_df.head()

"""## Shifts"""

def scrapeShifts(gameId):

    """
    Scrapes shift data from the NHL website for a given game ID.

    Parameters :
      - game_id (int) : The ID of the game you want to scrape the shift data for.

    Returns :
      - shifts_df (pd.DataFrame) : A DataFrame containing the scraped shift data.

    """


    url = f"https://api.nhle.com/stats/rest/en/shiftcharts?cayenneExp=gameId={gameId}"

    response = requests.get(url).json()

    shifts_df = pd.json_normalize(response["data"])

    # Add meta data (datetime of the execution)
    shifts_df["meta_datetime"] = pd.to_datetime("now")

    return shifts_df

shifts_df = scrapeShifts(2024020170)
shifts_df.head()

### Merge pbp and shifts

pbp_df

"""# Standings"""

def scrapeStandings(date):
    """
    Scrapes standings data from the NHL website for a given date.

    Parameters :
      - date (str) : The date you want to scrape the standings data for in the format of "YYYY-MM-DD".

    Returns :
      - standings_df (pd.DataFrame) : A DataFrame containing the scraped standings data.

    """



    url = f"https://api-web.nhle.com/v1/standings/{date}"

    response = requests.get(url).json()

    standings_df = pd.json_normalize(response["standings"])

    # Add meta data (datetime of the execution)
    standings_df["meta_datetime"] = pd.to_datetime("now")

    return standings_df

standings_df = scrapeStandings("2024-11-06")
standings_df.head()

"""# Player"""

def scrapePlayer(playerId, key=None):

    """
    Scrapes player data from the NHL website for a given player ID.

    Parameters :
      - playerId (int) : The ID of the player you want to scrape the data for.
      - key (str) : The key to use to extract the data from the response.

    Returns :
      - response (dict) : A dictionary containing the scraped player data.

      Data in dict :
        - playerId
        - isActive
        - currentTeamId
        - currentTeamAbbrev
        - fullTeamName
        - teamCommonName
        - teamPlaceNameWithPreposition
        - firstName
        - lastName
        - teamLogo
        - sweaterNumber
        - position
        - headshot
        - heroImage
        - heightInInches
        - heightInCentimeters
        - weightInPounds
        - weightInKilograms
        - birthDate
        - birthCity
        - birthStateProvince
        - birthCountry
        - shootsCatches
        - draftDetails
        - playerSlug
        - inTop100AllTime
        - inHHOF
        - featuredStats
        - careerTotals
        - shopLink
        - twitterLink
        - watchLink
        - last5Games
        - seasonTotals
        - currentTeamRoster

    """


    url = f"https://api-web.nhle.com/v1/player/{playerId}/landing"

    response = requests.get(url).json()

    if key in response.keys():
        response = response[key]
    else:
        response = response

    return response

pd.json_normalize(scrapePlayer(8481540, 'seasonTotals'))

info = scrapePlayer(8481540)

info['firstName']['default'], info['lastName']['default'], info['position']

"""# NHLEdge

## Skater

### Overview
"""

def get_player_overview(season, session, playerId, include_info = True):

    """

    Scrapes overview data from the NHL website (NHL Edge) for a given player.

    Parameters :
      - season (str/int) : The season you want to scrape the overview data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the overview data for.
      - playerId (int) : The ID of the player you want to scrape the overview data for.

    Returns :
      - overview_df (pd.DataFrame) : A DataFrame containing the scraped overview data.

      * Percentile is assuming the size of the radar is 90 units.

    """



    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Construct the URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"

    # Open the page
    driver.get(url)

    # Define headers and metrics
    headers = ['Metric', 'Value', 'League average by position (F/D)', 'Percentile']
    metrics = ['Top Skating Speed (mph)', 'Speed Bursts Over 20 mph', "Skating Distance (mi)",
               "Top Shot Speed (mph)", "Shots on Goal", "Shooting %", "Goals", "Off. Zone Time (ES)"]


    # Initialize the dictionary to hold metrics
    metrics_dict = {}

    try:
        # Wait until the JavaScript-rendered element with data-section="overview" loads
        overview_section = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, '[data-section="overview"]'))
        )

        # Get the text from the overview section
        overview_text = overview_section.text

        # Wait until the JavaScript-rendered elements with the specified class are loaded
        radar_elements = WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.sl-webc__radar-chart__area-datum'))
        )

        # Loop through each radar element and associated metric
        for element, metric in zip(radar_elements, metrics):
            # Initialize dictionary for each metric if not already done
            metrics_dict[metric] = {}

            # Get the cx and cy attributes, and convert them to float
            metrics_dict[metric]["x"] = float(element.get_attribute("cx"))
            metrics_dict[metric]["y"] = float(element.get_attribute("cy"))

    finally:
        # Close the driver
        driver.quit()



    # Process each line to split the values and prepare for DataFrame
    data = overview_text.split("\n")[3:11]
    processed_data = []
    for line, metric in zip(data, metrics):
        line1 = line.strip(metric + " ")
        parts = line1.split(" ")
        val = float(parts[0].replace("%", ""))/100 if "%" in parts[0] else float(parts[0])
        pos_avg = float(parts[1].replace("%", ""))/100 if "%" in parts[1] else float(parts[1])
        percentile = "Below 50th" if parts[2].isalpha() else float(parts[2])

        # Append processed row
        processed_data.append([metric, val, pos_avg, percentile])

    # Create DataFrame
    overview_df = pd.DataFrame(processed_data, columns=headers)

    ov_df = pd.DataFrame(metrics_dict).T
    ov_df['Distance from centre'] = np.sqrt((ov_df["x"] - 0)**2 + (ov_df["y"] - 0)**2)
    ov_df["Percentile estimation"] = np.minimum((ov_df['Distance from centre'] / 90) * 100, 100)
    ov_df = ov_df.reset_index().rename(columns={"index": "Metric"})

    # Join ov_df on overview_df on 'Label' column
    overview_df = overview_df.merge(ov_df, on='Metric', how='left')

    if include_info:
        info = scrapePlayer(playerId)
        overview_df['firstName'] = info['firstName']['default']
        overview_df['lastName'] = info['lastName']['default']
        overview_df['fullName'] = info['firstName']['default'] + " " + info['lastName']['default']
        overview_df['position'] = info['position']
    # Identifyer columns
    overview_df["season"] = season
    overview_df["session"] = session
    overview_df["playerId"] = playerId



    # Add meta data (datetime of the execution)
    overview_df["meta_datetime"] = pd.to_datetime("now")

    return overview_df

get_player_overview(20242025, "regular", 8473512)

"""### Speed"""

def get_player_speed(season, session, playerId):

    """

    Scrapes speed data from the NHL website (NHL Edge) for a given player.

    Parameters:
      - season (str/int) : The season you want to scrape the speed data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the speed data for.
      - playerId (int) : The ID of the player you want to scrape the speed data for.

    Returns:
      - speed_df (pd.DataFrame) : A DataFrame containing the scraped speed data.

    """


    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"


    # Open the page
    driver.get(url)

    try:
        # Wait until the JavaScript-rendered element with data-section="overview" loads
        speed_section = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="skatingspeed-section-content"]/div[1]/div/table/tbody'))
        )
        # Get the text from the overview section
        speed_text = speed_section.text
        # print(speed_text)
    finally:
        # Close the driver
        driver.quit()

    headers = ['Metric', 'Value', 'League average by position (F/D)', 'Percentile']
    metrics = ['Top Speed (mph)', "22+ mph bursts", "20-22 mph bursts", "18-20 mph bursts"]
    data = speed_text.split("\n")

    # Process each line to split the values and prepare for DataFrame
    processed_data = []
    for line, metric  in zip(data, metrics):
        line1 = line.replace(metric + " ", "")
        parts = line1.split(" ")
        val = float(parts[0].replace("%", ""))/100 if "%" in parts[0] else float(parts[0])
        pos_avg = float(parts[1].replace("%", ""))/100 if "%" in parts[1] else float(parts[1])
        percentile = "Below 50th" if parts[2].isalpha() else float(parts[2])

        # Append processed row
        processed_data.append([metric, val, pos_avg, percentile])

    # Create DataFrame
    speed_df = pd.DataFrame(processed_data, columns=headers)

    # Identifyer columns
    speed_df["season"] = season
    speed_df["session"] = session
    speed_df["playerId"] = playerId

    # Add meta data (datetime of the execution)
    speed_df["meta_datetime"] = pd.to_datetime("now")

    return speed_df


get_player_speed(20242025, "regular", 8473512)

"""### Distance skated"""

def get_player_distance_overview(season, session, playerId, include_info=True):

    """

    Scrapes distance skated data from the NHL website (NHL Edge) for a given player.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - playerId (int) : The ID of the player you want to scrape the distance skated data for.

    Returns:
      - distance_df (pd.DataFrame) : A DataFrame containing the scraped distance skated data.

    """



    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"

    # Open the page
    driver.get(url)

    # Define the XPaths for each strength type button
    strength_buttons = {
        "All Strengths": '//*[@id="manpower-selector-skatingdistance"]/label[1]',
        "Even Strength": '//*[@id="manpower-selector-skatingdistance"]/label[2]',
        "Power Play": '//*[@id="manpower-selector-skatingdistance"]/label[3]',
        "Penalty Kill": '//*[@id="manpower-selector-skatingdistance"]/label[4]'
    }

    headers = ['Metric', 'Value', 'League average by position (F/D)', 'Percentile']
    metrics = ['Total (mi)', "Average Per 60 (mi)", "Top Game (mi)", "Top Period (mi)"]

    df_list = []
    failed_list = []
    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for strength, xpath in strength_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
                # Wait until the data content is visible and contains updated content
                data_section_xpath = '//*[@id="skatingdistance-section-content"]/div[1]/div/table/tbody'
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, data_section_xpath))
                )

                # Extract and print data for the current strength type
                distance_section = driver.find_element(By.XPATH, data_section_xpath)
                distance_text = distance_section.text
                # print(f"{strength} Stats:\n", distance_text, "\n")
                time.sleep(3)  # Short delay before moving to the next button

                data = distance_text.split("\n")
                processed_data = []
                for line, metric  in zip(data, metrics):
                    line1 = line.replace(metric + " ", "")
                    parts = line1.split(" ")
                    val = float(parts[0].replace("%", ""))/100 if "%" in parts[0] else float(parts[0])
                    pos_avg = float(parts[1].replace("%", ""))/100 if "%" in parts[1] else float(parts[1])
                    percentile = "Below 50th" if parts[2].isalpha() else float(parts[2])

                    # Append processed row
                    processed_data.append([metric, val, pos_avg, percentile])


                # Create DataFrame
                distance_df = pd.DataFrame(processed_data, columns=headers)
                distance_df['Strength'] = strength
                # print(distance_df)

                df_list.append(distance_df)



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {strength}: {inner_e}")
                failed_list.append(strength)

    except Exception as e:
        print("An error occurred:", str(e))

    finally:
        driver.quit()
        distance_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None

    if include_info:
        info = scrapePlayer(playerId)
        distance_dfs['firstName'] = info['firstName']['default']
        distance_dfs['lastName'] = info['lastName']['default']
        distance_dfs['fullName'] = info['firstName']['default'] + " " + info['lastName']['default']
        distance_dfs['position'] = info['position']

    distance_dfs["season"] = season
    distance_dfs["session"] = session
    distance_dfs["playerId"] = playerId

    # Add meta data (datetime of the execution)
    distance_dfs["meta_datetime"] = pd.to_datetime("now")

    return distance_dfs

get_player_distance_overview(20242025, "regular", 8473512)

def get_player_distance(season, session, playerId, include_info=True):

    """

    Scrapes distance skated per game data from the NHL website (NHL Edge) for a given player.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - playerId (int) : The ID of the player you want to scrape the distance skated data for.


    Returns:
      - dis_dfs (pd.DataFrame) : A DataFrame containing the scraped distance skated per game data.

    """


    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"

    # Open the page
    driver.get(url)

    # Define the XPaths for each strength type button
    strength_buttons = {
        "All Strengths": '//*[@id="manpower-selector-skatingdistance"]/label[1]',
        "Even Strength": '//*[@id="manpower-selector-skatingdistance"]/label[2]',
        "Power Play": '//*[@id="manpower-selector-skatingdistance"]/label[3]',
        "Penalty Kill": '//*[@id="manpower-selector-skatingdistance"]/label[4]'
    }

    # headers = ['Label', 'Value', 'League average by position (F/D)', 'Percentile']
    # metrics = ['Total (mi)', "Average Per 60 (mi)", "Top Game (mi)", "Top Period (mi)"]

    df_list = []
    failed_list = []

    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for strength, xpath in strength_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
              # Wait until the element with the data-json attribute is present
              chart_element = WebDriverWait(driver, 10).until(
                  EC.presence_of_element_located((By.XPATH, '//*[@id="skatingdistance-datebarchart"]'))
              )


              # Extract the data-json attribute
              data_json = chart_element.get_attribute("data-json")

              # Parse the JSON data if you need it as a dictionary
              data_dict = json.loads(data_json)

              # Print the extracted data
              # print("Extracted JSON data:", data_dict)


              dis_df = pd.json_normalize(data_dict['chartData'])
              dis_df['Distance skated (mi)'] = [float(data_dict['chartData'][i]["tooltip"][0].replace("Distance skated: ", "").replace(" miles", "")) for i in range(len(data_dict['chartData']))]
              dis_df['opponent'] = [data_dict['chartData'][i]["tooltip"][1].split(" @ ")[1] if " @ " in data_dict['chartData'][i]["tooltip"][1] else data_dict['chartData'][i]["tooltip"][1].split(" vs ")[1] for i in range(len(data_dict['chartData']))]
              dis_df["Home/Away"] = ["Home" if " vs "  in data_dict['chartData'][i]["tooltip"][1] else "Away" for i in range(len(data_dict['chartData']))]
              dis_df["TOI"] = [data_dict['chartData'][i]["tooltip"][2].replace("TOI: ", "") for i in range(len(data_dict['chartData']))]
              dis_df = dis_df.drop(columns=['tooltip', 'value'])

              dis_df['Strength'] = strength
              # print(dis_df)

              df_list.append(dis_df)
              # time.sleep(5)  # Short delay before moving to the next button



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {strength}: {inner_e}")
                failed_list.append(strength)

    except Exception as e:
        print('Error with', strength.upper(), ':', str(e))


    finally:
        driver.quit()

    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None

    dis_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    if include_info:
        info = scrapePlayer(playerId)
        dis_dfs['firstName'] = info['firstName']['default']
        dis_dfs['lastName'] = info['lastName']['default']
        dis_dfs['fullName'] = info['firstName']['default'] + " " + info['lastName']['default']
        dis_dfs['position'] = info['position']

    dis_dfs["season"] = season
    dis_dfs["session"] = session
    dis_dfs["playerId"] = playerId

    # Add meta data (datetime of the execution)
    dis_dfs["meta_datetime"] = pd.to_datetime("now")

    return dis_dfs


get_player_distance(20242025, "regular", 8476875)

"""### Shot speed"""

def get_shot_speed(season, session, playerId, include_info=True):

    """

    Scrapes shot speed data from the NHL website (NHL Edge) for a given player.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - playerId (int) : The ID of the player you want to scrape the distance skated data for.

    Returns:
      - shot_speed_df (pd.DataFrame) : A DataFrame containing the scraped shot speed data.


    """

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"


    # Open the page
    driver.get(url)

    try:
        # Wait until the JavaScript-rendered element with data-section="overview" loads
        shot_speed_section = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="shotspeed-section-content"]/div[1]/div/table'))
        )
        # Get the text from the overview section
        shot_speed_text = shot_speed_section.text

        data = shot_speed_text.split("\n")[2:]
        headers = ['Metric', 'Value', 'League average by position (F/D)', 'Percentile']
        metrics = ['Top Speed (mph)', 'Average Speed (mph)', '100+ mph shots', '90-100 mph shots', '80-90 mph shots', '70-80 mph shots']

        processed_data = []
        for line, metric  in zip(data, metrics):
            line1 = line.replace(metric + " ", "")
            parts = line1.split(" ")
            league_avg = parts[0]
            pos_avg = parts[1]
            percentile = "Below 50th" if parts[2].isalpha() else parts[2]

            # Append processed row
            processed_data.append([metric, league_avg, pos_avg, percentile])

        # Create DataFrame
        shot_speed_df = pd.DataFrame(processed_data, columns=headers)

        if include_info:
            info = scrapePlayer(playerId)
            shot_speed_df['firstName'] = info['firstName']['default']
            shot_speed_df['lastName'] = info['lastName']['default']
            shot_speed_df['fullName'] = info['firstName']['default'] + " " + info['lastName']['default']
            shot_speed_df['position'] = info['position']

        shot_speed_df["season"] = season
        shot_speed_df["session"] = session
        shot_speed_df["playerId"] = playerId

        # Add meta data (datetime of the execution)
        shot_speed_df["meta_datetime"] = pd.to_datetime("now")


    finally:
        # Close the driver
        driver.quit()


    return shot_speed_df

get_shot_speed(20242025, "regular", 8473512)

"""### Shot locations"""

def get_shot_location_overview(season, session, playerId, include_info=True):

    """

    Scrapes shot location overview data from the NHL website (NHL Edge) for a given player.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - playerId (int) : The ID of the player you want to scrape the distance skated data for.

    Returns:
      - shot_location_df (pd.DataFrame) : A DataFrame containing the scraped shot location overview data.


    """


    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"

    # Open the page
    driver.get(url)



    location_buttons = {
        "All locations" : '//*[@id="shotlocation-selector-shotlocation"]/label[1]',
        "High-Danger" : '//*[@id="shotlocation-selector-shotlocation"]/label[2]',
        "Mid-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[3]',
        "Long-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[4]'

    }

    headers = ['Metric', 'Value', 'League average by position (F/D)', 'Percentile']
    metrics = ['Shots on Goal', "Goals", "Shooting %"]

    df_list = []
    failed_list = []
    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for location, xpath in location_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
                # Wait until the data content is visible and contains updated content
                data_section_xpath = '//*[@id="shotlocation-section-content"]/div[1]/div/table/tbody'
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, data_section_xpath))
                )

                # Extract and print data for the current strength type
                shot_location_section = driver.find_element(By.XPATH, data_section_xpath)
                shot_location_text = shot_location_section.text
                # print(f"{strength} Stats:\n", distance_text, "\n")
                time.sleep(3)  # Short delay before moving to the next button

                data = shot_location_text.split("\n")
                processed_data = []
                for line, metric  in zip(data, metrics):
                    line1 = line.replace(metric + " ", "")
                    parts = line1.split(" ")
                    val = float(parts[0].replace("%", ""))/100 if "%" in parts[0] else parts[0]
                    pos_avg = float(parts[1].replace("%", ""))/100 if "%" in parts[1] else parts[1]
                    percentile = "Below 50th" if parts[2].isalpha() else parts[2]

                    # Append processed row
                    processed_data.append([metric, val, pos_avg, percentile])

                # Create DataFrame
                shot_location_df = pd.DataFrame(processed_data, columns=headers)
                shot_location_df['Shot location'] = location
                # print(distance_df)

                df_list.append(shot_location_df)
                # print(data)



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {location}: {inner_e}")
                failed_list.append(location)

    except Exception as e:
        print("An error occurred:", str(e))

    finally:
        driver.quit()

    shot_location_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None

    if include_info:
        info = scrapePlayer(playerId)
        shot_location_dfs['firstName'] = info['firstName']['default']
        shot_location_dfs['lastName'] = info['lastName']['default']
        shot_location_dfs['fullName'] = info['firstName']['default'] + " " + info['lastName']['default']
        shot_location_dfs['position'] = info['position']

    shot_location_dfs["season"] = season
    shot_location_dfs["session"] = session
    shot_location_dfs["playerId"] = playerId

    # Add meta data (datetime of the execution)
    shot_location_dfs["meta_datetime"] = pd.to_datetime("now")

    return shot_location_dfs



get_shot_location_overview(20232024, "regular", 8473512)

def get_shot_location(season, session, playerId, include_info=True):

    """

    Scrapes shot location data from the NHL website (NHL Edge) for a given player.


    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - playerId (int) : The ID of the player you want to scrape the distance skated data for.



    Returns:
      - shot_location_df (pd.DataFrame) : A DataFrame containing the scraped shot location data.

    """


    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"

    # Open the page
    driver.get(url)

    # Define the XPaths for each metric type button
    metric_buttons = {
        "Shots on Goal": '//*[@id="shootingmetrics-selector-shotlocation"]/label[1]',
        "Goals": '//*[@id="shootingmetrics-selector-shotlocation"]/label[2]',
        "Shooting%": '//*[@id="shootingmetrics-selector-shotlocation"]/label[3]'
    }

    location_buttons = {
        "All locations" : '//*[@id="shotlocation-selector-shotlocation"]/label[1]',
        "High-Danger" : '//*[@id="shotlocation-selector-shotlocation"]/label[2]',
        "Mid-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[3]',
        "Long-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[4]'

    }



    df_list = []
    failed_list = []

    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for metric, xpath_metric in metric_buttons.items():
            for location, xpath_location in location_buttons.items():


                # Locate the button each time in the loop to avoid stale references
                button_metric = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, xpath_metric))
                )

                # Scroll the button into view and click
                driver.execute_script("arguments[0].scrollIntoView(true);", button_metric)
                time.sleep(3)  # Short delay for stability
                driver.execute_script("arguments[0].click();", button_metric)


                # Locate the button each time in the loop to avoid stale references
                button_location = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, xpath_location))
                )

                # Scroll the button into view and click
                driver.execute_script("arguments[0].scrollIntoView(true);", button_location)
                time.sleep(3)  # Short delay for stability
                driver.execute_script("arguments[0].click();", button_location)

                # Reinitialize the data section locator and wait for it to update
                try:
                  # Wait until the element with the data-json attribute is present
                  chart_element = WebDriverWait(driver, 10).until(
                      EC.presence_of_element_located((By.XPATH, '//*[@id="shotlocation-shotchart"]'))
                  )


                  # Extract the data-json attribute
                  data_json = chart_element.get_attribute("data-json")

                  # Parse the JSON data if you need it as a dictionary
                  data_dict = json.loads(data_json)

                  # Print the extracted data
                  # print("Extracted JSON data:", data_dict)

                  sl_df = pd.json_normalize(data_dict['chartData'])
                  sl_df['Location'] = location
                  sl_df['Metric'] = metric

                  df_list.append(sl_df)
                  time.sleep(5)  # Short delay before moving to the next button






                except Exception as inner_e:
                    print(f"An error occurred while trying to retrieve stats for {metric} - {location}: {inner_e}")
                    failed_list.append((metric, location))

    except Exception as e:
        print('Error with', metric, "-", location, ':', str(e))


    finally:
        driver.quit()


    sl_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    if include_info:
        info = scrapePlayer(playerId)
        sl_dfs['firstName'] = info['firstName']['default']
        sl_dfs['lastName'] = info['lastName']['default']
        sl_dfs['fullName'] = info['firstName']['default'] + " " + info['lastName']['default']
        sl_dfs['position'] = info['position']


    sl_dfs["season"] = season
    sl_dfs["session"] = session
    sl_dfs["playerId"] = playerId

    # Add meta data (datetime of the execution)
    sl_dfs["meta_datetime"] = pd.to_datetime("now")


    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None

    return sl_dfs

get_shot_location(20232024, "regular", 8473512)

"""### Zone TOI"""

def get_zone_toi(season, session, playerId, include_info=True):

    """

    Scrapes zone TOI % per strength data from the NHL website (NHL Edge) for a given player.


    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - playerId (int) : The ID of the player you want to scrape the distance skated data for.

    Returns:
      - zone_toi_df (pd.DataFrame) : A DataFrame containing the scraped zone TOI data.

    """




    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/skater/{season}-{session}-{playerId}"

    # Open the page
    driver.get(url)

    # Define the XPaths for each strength type button
    strength_buttons = {
        "All Strengths": '//*[@id="manpower-selector-zonetime"]/label[1]',
        "Even Strength": '//*[@id="manpower-selector-zonetime"]/label[2]',
        "Power Play": '//*[@id="manpower-selector-zonetime"]/label[3]',
        "Penalty Kill": '//*[@id="manpower-selector-zonetime"]/label[4]'
    }

    headers = ['Zone', 'Value', 'League average by position (F/D)', 'Percentile']
    metrics = ['Offensive Zone', "Neutral Zone", "Defensive Zone"]

    df_list = []
    failed_list = []

    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for strength, xpath in strength_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
                # Wait until the data content is visible and contains updated content
                data_section_xpath = '//*[@id="zonetime-section-content"]/div[1]/div/table/tbody'
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, data_section_xpath))
                )

                # Extract and print data for the current strength type
                zone_time_section = driver.find_element(By.XPATH, data_section_xpath)
                zone_time_text = zone_time_section.text
                # print(f"{strength} Stats:\n", distance_text, "\n")
                time.sleep(3)  # Short delay before moving to the next button

                data = zone_time_text.split("\n")
                processed_data = []
                for line, metric  in zip(data, metrics):
                    line1 = line.replace(metric + " ", "")
                    parts = line1.split(" ")
                    val = float(parts[0].replace("%", ""))/100 if "%" in parts[0] else parts[0]
                    pos_avg = float(parts[1].replace("%", ""))/100 if "%" in parts[1] else parts[1]
                    percentile = "Below 50th" if parts[2].isalpha() else parts[2]

                    # Append processed row
                    processed_data.append([metric, val, pos_avg, percentile])

                # Create DataFrame
                zone_time_df = pd.DataFrame(processed_data, columns=headers)
                zone_time_df['Strength'] = strength
                # print(distance_df)

                df_list.append(zone_time_df)



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {strength}: {inner_e}")
                failed_list.append(strength)

    except Exception as e:
        print("An error occurred:", str(e))

    finally:
        driver.quit()
    zone_time_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None

    if include_info:
        info = scrapePlayer(playerId)
        zone_time_dfs['firstName'] = info['firstName']['default']
        zone_time_dfs['lastName'] = info['lastName']['default']
        zone_time_dfs['fullName'] = info['firstName']['default'] + " " + info['lastName']['default']
        zone_time_dfs['position'] = info['position']

    zone_time_dfs["season"] = season
    zone_time_dfs["session"] = session
    zone_time_dfs["playerId"] = playerId

    # Add meta data (datetime of the execution)
    zone_time_dfs["meta_datetime"] = pd.to_datetime("now")

    return zone_time_dfs


get_zone_toi(20232024, "regular", 8473512)

"""## Team

### Overview
"""

def get_team_overview(season, session, teamSlug):

    """

    Scrapes overview data from the NHL website (NHL Edge) for a given team.


    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - overview_df (pd.DataFrame) : A DataFrame containing the scraped overview data.

    """

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"

    # Open the page
    driver.get(url)

    headers = ['Metric', 'Value', 'League average', 'Percentile']
    metrics = ["Top Skating Speed (mph)", "Speed Bursts Over 20 mph", "Skating Distance (mi)", "Top Shot Speed (mph)", "Shots on Goal", "Shooting %", "Goals", "Off. Zone Time (ES)"]

    df_list = []

    metrics_dict = {}
    try:
        # Wait until the JavaScript-rendered element with data-section="overview" loads
        overview_section = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="overview-section-content"]/div[1]/div/table/tbody'))
        )
        # Get the text from the overview section
        overview_text = overview_section.text
        data = overview_text.split("\n")
        processed_data = []
        for line, metric  in zip(data, metrics):
            # print(line)
            line1 = line.replace(metric + " ", "").replace(",", "")
            parts = line1.split(" ")
            val = float(parts[0].replace("%", "")) if "%" in parts[0] else float(parts[0])
            pos_avg = float(parts[1].replace("%", "")) if "%" in parts[1] else float(parts[1])
            percentile = "Below 50th" if parts[2].isalpha() else float(parts[2])

            # Append processed row
            processed_data.append([metric, val, pos_avg, percentile])

        # Create DataFrame
        overview_df = pd.DataFrame(processed_data, columns=headers)

        df_list.append(overview_df)

        # Wait until the JavaScript-rendered elements with the specified class are loaded
        radar_elements = WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, '.sl-webc__radar-chart__area-datum'))
        )

        # Loop through each radar element and associated metric
        for element, metric in zip(radar_elements, metrics):
            # Initialize dictionary for each metric if not already done
            metrics_dict[metric] = {}

            # Get the cx and cy attributes, and convert them to float
            metrics_dict[metric]["x"] = float(element.get_attribute("cx"))
            metrics_dict[metric]["y"] = float(element.get_attribute("cy"))


    finally:
        # Close the driver
        driver.quit()

    overview_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    overview_dfs["season"] = season
    overview_dfs["session"] = session
    overview_dfs["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    overview_dfs["meta_datetime"] = pd.to_datetime("now")


    ov_df = pd.DataFrame(metrics_dict).T
    ov_df['Distance from centre'] = np.sqrt((ov_df["x"] - 0)**2 + (ov_df["y"] - 0)**2)
    ov_df["Percentile estimation"] = np.minimum((ov_df['Distance from centre'] / 90) * 100, 100)

    ov_df = ov_df.reset_index().rename(columns={'index': 'Metric'})
    overview_dfs = overview_dfs.merge(ov_df, on='Metric', how="left")


    return overview_dfs

get_team_overview(20242025, "regular", "MTL").drop(columns=["x", "y", "Distance from centre"])

(get_team_overview(20242025, "regular", "MTL")
# Drop useless columns
.drop(columns=["x", "y", "Distance from centre"])
#Style the dataframe
.style.background_gradient(cmap='Reds', subset="Percentile estimation", vmin=0, vmax=100))

"""### Skating Speed"""

def get_skating_speed(season, session, teamSlug):

    """

    Scrapes skating speed data from the NHL website (NHL Edge) for a given team.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - speed_df (pd.DataFrame) : A DataFrame containing the scraped speed data.

    """



    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"


    # Open the page
    driver.get(url)

    headers = ['Metric', 'Value', 'League average', 'Percentile']
    metrics = ['Top Speed (mph)', "22+ mph bursts", "20-22 mph bursts", "18-20 mph bursts"]

    try:
        # Wait until the JavaScript-rendered element with data-section="overview" loads
        speed_section = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="skatingspeed-section-content"]/div[1]/div/table/tbody'))
        )
        # Get the text from the overview section
        speed_text = speed_section.text

        data = speed_text.split("\n")

        # Process each line to split the values and prepare for DataFrame
        processed_data = []
        for line, metric  in zip(data, metrics):
            line1 = line.replace(metric + " ", "").replace(",", "")
            parts = line1.split(" ")
            val = float(parts[0])
            league_avg = float(parts[1])
            percentile = "Below 50th" if parts[2].isalpha() else float(parts[2])

            # Append processed row
            processed_data.append([metric, val, league_avg, percentile])

        # Create DataFrame
        speed_df = pd.DataFrame(processed_data, columns=headers)


    finally:
        # Close the driver
        driver.quit()


    speed_df["season"] = season
    speed_df["session"] = session
    speed_df["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    speed_df["meta_datetime"] = pd.to_datetime("now")

    return speed_df

get_skating_speed(20232024, "regular", "MTL")

"""### Skating Distance"""

def get_skating_distance_overview(season, session, teamSlug):

    """

    Scrapes distance skated data from the NHL website (NHL Edge) for a given team.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - distance_df (pd.DataFrame) : A DataFrame containing the scraped distance skated data.

    """

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"

    # Open the page
    driver.get(url)

    # Define the XPaths for each strength type button
    strength_buttons = {
        "All Strengths": '//*[@id="manpower-selector-skatingdistance"]/label[1]',
        "Even Strength": '//*[@id="manpower-selector-skatingdistance"]/label[2]',
        "Power Play": '//*[@id="manpower-selector-skatingdistance"]/label[3]',
        "Penalty Kill": '//*[@id="manpower-selector-skatingdistance"]/label[4]'
    }

    headers = ['Label', 'Value', 'League average', 'Percentile']
    metrics = ['Total (mi)', "Average Per 60 (mi)", "Top Game (mi)", "Top Period (mi)"]

    df_list = []
    failed_list = []

    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for strength, xpath in strength_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
                # Wait until the data content is visible and contains updated content
                data_section_xpath = '//*[@id="skatingdistance-section-content"]/div[1]/div/table/tbody'
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, data_section_xpath))
                )

                # Extract and print data for the current strength type
                distance_section = driver.find_element(By.XPATH, data_section_xpath)
                distance_text = distance_section.text
                # print(f"{strength} Stats:\n", distance_text, "\n")
                time.sleep(3)  # Short delay before moving to the next button

                data = distance_text.split("\n")
                processed_data = []
                for line, metric  in zip(data, metrics):
                    line1 = line.replace(metric + " ", "").replace(",", "")
                    parts = line1.split(" ")
                    val = float(parts[0])
                    league_avg = float(parts[1])
                    percentile = "Below 50th" if parts[2].isalpha() else float(parts[2])

                    # Append processed row
                    processed_data.append([metric, val, league_avg, percentile])

                # Create DataFrame
                distance_df = pd.DataFrame(processed_data, columns=headers)
                distance_df['Strength'] = strength
                # print(distance_df)

                df_list.append(distance_df)



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {strength}: {inner_e}")
                failed_list.append(strength)

    except Exception as e:
        print("An error occurred:", str(e))

    finally:
        driver.quit()
    distance_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)
    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None

    distance_dfs["season"] = season
    distance_dfs["session"] = session
    distance_dfs["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    distance_dfs["meta_datetime"] = pd.to_datetime("now")

    return distance_dfs

get_skating_distance_overview(20232024, "regular", "MTL")

def get_skating_distance(season, session, teamSlug):

    """

    Scrapes distance skated data from the NHL website (NHL Edge) for a given team.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - dis_dfs (pd.DataFrame) : A DataFrame containing the scraped distance skated data.

    """

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"

    # Open the page
    driver.get(url)

    # Define the XPaths for each strength type button
    strength_buttons = {
        "All Strengths": '//*[@id="manpower-selector-skatingdistance"]/label[1]',
        "Even Strength": '//*[@id="manpower-selector-skatingdistance"]/label[2]',
        "Power Play": '//*[@id="manpower-selector-skatingdistance"]/label[3]',
        "Penalty Kill": '//*[@id="manpower-selector-skatingdistance"]/label[4]'
    }

    df_list = []
    failed_list = []

    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for strength, xpath in strength_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
              # Wait until the element with the data-json attribute is present
              chart_element = WebDriverWait(driver, 10).until(
                  EC.presence_of_element_located((By.XPATH, '//*[@id="skatingdistance-datebarchart"]'))
              )


              # Extract the data-json attribute
              data_json = chart_element.get_attribute("data-json")

              # Parse the JSON data if you need it as a dictionary
              data_dict = json.loads(data_json)

              # Print the extracted data
              # print("Extracted JSON data:", data_dict)


              dis_df = pd.json_normalize(data_dict['chartData'])
              dis_df['Distance skated (mi)'] = [float(data_dict['chartData'][i]["tooltip"][0].replace("Distance skated: ", "").replace(" miles", "")) for i in range(len(data_dict['chartData']))]
              dis_df['opponent'] = [data_dict['chartData'][i]["tooltip"][1].split(" @ ")[1] if " @ " in data_dict['chartData'][i]["tooltip"][1] else data_dict['chartData'][i]["tooltip"][1].split(" vs ")[1] for i in range(len(data_dict['chartData']))]
              dis_df["Home/Away"] = ["Home" if " vs "  in data_dict['chartData'][i]["tooltip"][1] else "Away" for i in range(len(data_dict['chartData']))]
              dis_df["TOI"] = [data_dict['chartData'][i]["tooltip"][2].replace("TOI: ", "") for i in range(len(data_dict['chartData']))]
              dis_df = dis_df.drop(columns=['tooltip', 'value'])

              dis_df['Strength'] = strength
              # print(dis_df)

              df_list.append(dis_df)
              # time.sleep(5)  # Short delay before moving to the next button



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {strength}: {inner_e}")
                failed_list.append(strength)

    except Exception as e:
        print('Error with', strength.upper(), ':', str(e))


    finally:
        driver.quit()

    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None
    dis_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    dis_dfs["season"] = season
    dis_dfs["session"] = session
    dis_dfs["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    dis_dfs["meta_datetime"] = pd.to_datetime("now")

    return dis_dfs

get_skating_distance(20232024, "regular", "MTL")

"""### Shot speed"""

def get_team_shot_speed(season, session, teamSlug):

    """

    Scrapes shot speed data from the NHL website (NHL Edge) for a given team.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - speed_df (pd.DataFrame) : A DataFrame containing the scraped speed data.

    """

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"


    # Open the page
    driver.get(url)

    try:
        # Wait until the JavaScript-rendered element with data-section="overview" loads
        shot_speed_section = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, '//*[@id="shotspeed-section-content"]/div[1]/div/table/tbody'))
        )
        # Get the text from the overview section
        shot_speed_text = shot_speed_section.text

        data = shot_speed_text.split("\n")
        # print(data)

        # Process each line to split the values and prepare for DataFrame
        headers = ['Metric', 'Value', 'League average', 'Percentile']
        metrics = ['Top Speed (mph)', 'Average Speed (mph)', '100+ mph shots', '90-100 mph shots', '80-90 mph shots', '70-80 mph shots']

        processed_data = []
        for line, metric  in zip(data, metrics):
            line1 = line.replace(metric + " ", "").replace(",", "")
            parts = line1.split(" ")
            val = float(parts[0])
            league_avg = float(parts[1])
            percentile = "Below 50th" if parts[2].isalpha() else float(parts[2])

            # Append processed row
            processed_data.append([metric,val, league_avg, percentile])

        # Create DataFrame
        shot_speed_df = pd.DataFrame(processed_data, columns=headers)





    finally:
        # Close the driver
        driver.quit()

    shot_speed_df["season"] = season
    shot_speed_df["session"] = session
    shot_speed_df["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    shot_speed_df["meta_datetime"] = pd.to_datetime("now")

    return shot_speed_df

get_team_shot_speed(20232024, "regular", "MTL")

def get_team_shot_location(season, session, teamSlug):

    """

    Scrapes shot location data from the NHL website (NHL Edge) for a given team.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - shot_location_dfs (pd.DataFrame) : A DataFrame containing the scraped shot location data.

    """



    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"

    # Open the page
    driver.get(url)



    location_buttons = {
        "All locations" : '//*[@id="shotlocation-selector-shotlocation"]/label[1]',
        "High-Danger" : '//*[@id="shotlocation-selector-shotlocation"]/label[2]',
        "Mid-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[3]',
        "Long-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[4]'

    }

    headers = ['Metric', 'Value', 'League average', 'Percentile']
    metrics = ['Shots on Goal', "Goals", "Shooting %"]

    df_list = []
    failed_list = []
    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for location, xpath in location_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
                # Wait until the data content is visible and contains updated content
                data_section_xpath = '//*[@id="shotlocation-section-content"]/div[1]/div/table/tbody'
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, data_section_xpath))
                )

                # Extract and print data for the current strength type
                shot_location_section = driver.find_element(By.XPATH, data_section_xpath)
                shot_location_text = shot_location_section.text
                # print(f"{strength} Stats:\n", distance_text, "\n")
                time.sleep(3)  # Short delay before moving to the next button

                data = shot_location_text.split("\n")
                processed_data = []
                for line, metric  in zip(data, metrics):
                    line1 = line.replace(metric + " ", "").replace(",", "")
                    parts = line1.split(" ")
                    val = float(parts[0].replace("%", ""))/100 if "%" in parts[0] else parts[0]
                    pos_avg = float(parts[1].replace("%", ""))/100 if "%" in parts[1] else parts[1]
                    percentile = "Below 50th" if parts[2].isalpha() else parts[2]

                    # Append processed row
                    processed_data.append([metric, val, pos_avg, percentile])

                # Create DataFrame
                shot_location_df = pd.DataFrame(processed_data, columns=headers)
                shot_location_df['Shot location'] = location
                # print(distance_df)

                df_list.append(shot_location_df)
                # print(data)



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {location}: {inner_e}")
                failed_list.append(location)

    except Exception as e:
        print("An error occurred:", str(e))

    finally:
        driver.quit()

    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None
    shot_location_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    shot_location_dfs["season"] = season
    shot_location_dfs["session"] = session
    shot_location_dfs["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    shot_location_dfs["meta_datetime"] = pd.to_datetime("now")

    return shot_location_dfs



get_team_shot_location(20232024, "regular", "MTL")

"""### Shots locations"""

def get_team_shot_location(season, session, teamSlug):

    """

    Scrapes shot location data from the NHL website (NHL Edge) for a given team.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - sl_dfs (pd.DataFrame) : A DataFrame containing the scraped shot location data.

    """

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"

    # Open the page
    driver.get(url)

    # Define the XPaths for each metric type button
    metric_buttons = {
        "Shots on Goal": '//*[@id="shootingmetrics-selector-shotlocation"]/label[1]',
        "Goals": '//*[@id="shootingmetrics-selector-shotlocation"]/label[2]',
        "Shooting%": '//*[@id="shootingmetrics-selector-shotlocation"]/label[3]'
    }

    location_buttons = {
        "All locations" : '//*[@id="shotlocation-selector-shotlocation"]/label[1]',
        "High-Danger" : '//*[@id="shotlocation-selector-shotlocation"]/label[2]',
        "Mid-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[3]',
        "Long-Range" : '//*[@id="shotlocation-selector-shotlocation"]/label[4]'

    }



    df_list = []
    failed_list = []
    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for metric, xpath_metric in metric_buttons.items():
            for location, xpath_location in location_buttons.items():


                # Locate the button each time in the loop to avoid stale references
                button_metric = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, xpath_metric))
                )

                # Scroll the button into view and click
                driver.execute_script("arguments[0].scrollIntoView(true);", button_metric)
                time.sleep(3)  # Short delay for stability
                driver.execute_script("arguments[0].click();", button_metric)


                # Locate the button each time in the loop to avoid stale references
                button_location = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, xpath_location))
                )

                # Scroll the button into view and click
                driver.execute_script("arguments[0].scrollIntoView(true);", button_location)
                time.sleep(3)  # Short delay for stability
                driver.execute_script("arguments[0].click();", button_location)

                # Reinitialize the data section locator and wait for it to update
                try:
                  # Wait until the element with the data-json attribute is present
                  chart_element = WebDriverWait(driver, 10).until(
                      EC.presence_of_element_located((By.XPATH, '//*[@id="shotlocation-shotchart"]'))
                  )


                  # Extract the data-json attribute
                  data_json = chart_element.get_attribute("data-json")

                  # Parse the JSON data if you need it as a dictionary
                  data_dict = json.loads(data_json)

                  # Print the extracted data
                  # print("Extracted JSON data:", data_dict)

                  sl_df = pd.json_normalize(data_dict['chartData'])
                  sl_df['Location'] = location
                  sl_df['Metric'] = metric

                  df_list.append(sl_df)
                  time.sleep(5)  # Short delay before moving to the next button



                except Exception as inner_e:
                    print(f"An error occurred while trying to retrieve stats for {metric} - {location}: {inner_e}")
                    failed_list.append((metric, location))

    except Exception as e:
        print('Error with', metric, "-", location, ':', str(e))


    finally:
        driver.quit()

    sl_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)
    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None

    sl_dfs["season"] = season
    sl_dfs["session"] = session
    sl_dfs["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    sl_dfs["meta_datetime"] = pd.to_datetime("now")

    return sl_dfs


get_team_shot_location(20232024, "regular", "MTL")

"""### Zone TOI"""

def get_team_zone_toi(season, session, teamSlug):

    """

    Scrapes zone TOI data from the NHL website (NHL Edge) for a given team.

    Parameters:
      - season (str/int) : The season you want to scrape the distance skated data for in the format of "YYYYYYYY".
      - session (str) : The session you want to scrape the distance skated data for.
      - teamSlug (str) : The slug of the team you want to scrape the distance skated data for.

    Returns:
      - zone_time_dfs (pd.DataFrame) : A DataFrame containing the scraped zone TOI data.

    """

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    # Example URL
    url = f"https://edge.nhl.com/en/team/{season}-{session}-{teamSlug}"


    # Open the page
    driver.get(url)

    # Define the XPaths for each strength type button
    strength_buttons = {
        "All Strengths": '//*[@id="manpower-selector-zonetime"]/label[1]',
        "Even Strength": '//*[@id="manpower-selector-zonetime"]/label[2]',
        "Power Play": '//*[@id="manpower-selector-zonetime"]/label[3]',
        "Penalty Kill": '//*[@id="manpower-selector-zonetime"]/label[4]'
    }

    headers = ['Zone', 'Value', 'League average', 'Percentile']
    metrics = ['Offensive Zone', "Neutral Zone", "Defensive Zone"]

    df_list = []
    failed_list = []
    try:
        # Try to close any overlay that might interfere with the click
        try:
            close_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.XPATH, '//*[@id="close-modal-button"]'))
            )
            close_button.click()
        except:
            pass  # Continue if no overlay is present

        # Loop through each button and gather data
        for strength, xpath in strength_buttons.items():
            # Locate the button each time in the loop to avoid stale references
            button = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.XPATH, xpath))
            )

            # Scroll the button into view and click
            driver.execute_script("arguments[0].scrollIntoView(true);", button)
            time.sleep(3)  # Short delay for stability
            driver.execute_script("arguments[0].click();", button)

            # Reinitialize the data section locator and wait for it to update
            try:
                # Wait until the data content is visible and contains updated content
                data_section_xpath = '//*[@id="zonetime-section-content"]/div[1]/div/table/tbody'
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.XPATH, data_section_xpath))
                )

                # Extract and print data for the current strength type
                zone_time_section = driver.find_element(By.XPATH, data_section_xpath)
                zone_time_text = zone_time_section.text
                # print(f"{strength} Stats:\n", distance_text, "\n")
                time.sleep(3)  # Short delay before moving to the next button

                data = zone_time_text.split("\n")
                processed_data = []
                for line, metric  in zip(data, metrics):
                    line1 = line.replace(metric + " ", "")
                    parts = line1.split(" ")
                    val = float(parts[0].replace("%", "")) if "%" in parts[0] else parts[0]
                    pos_avg = float(parts[1].replace("%", "")) if "%" in parts[1] else parts[1]
                    percentile = "Below 50th" if parts[2].isalpha() else parts[2]

                    # Append processed row
                    processed_data.append([metric, val, pos_avg, percentile])

                # Create DataFrame
                zone_time_df = pd.DataFrame(processed_data, columns=headers)
                zone_time_df['Strength'] = strength
                # print(distance_df)

                df_list.append(zone_time_df)



            except Exception as inner_e:
                print(f"An error occurred while trying to retrieve stats for {strength}: {inner_e}")
                failed_list.append(strength)

    except Exception as e:
        print("An error occurred:", str(e))

    finally:
        driver.quit()

    print(f"Failed to retrieve data for: {failed_list}") if len(failed_list) > 0 else None
    zone_time_dfs = pd.concat(df_list, ignore_index=True).reset_index(drop=True)

    zone_time_dfs["season"] = season
    zone_time_dfs["session"] = session
    zone_time_dfs["teamSlug"] = teamSlug

    # Add meta data (datetime of the execution)
    zone_time_dfs["meta_datetime"] = pd.to_datetime("now")

    return zone_time_dfs

get_team_zone_toi(20232024, "regular", "MTL")

"""# Records

## Teams
"""

def scrapeTeams1():

    """
    Scrapes team data from the NHL website (NHL Records) and returns a DataFrame.

    Returns:
    - team_df (pd.DataFrame): A DataFrame containing the scraped team data.
    """

    url = "https://records.nhl.com/site/api/franchise?include=teams.id&include=teams.active&include=teams.triCode&include=teams.placeName&include=teams.commonName&include=teams.fullName&include=teams.logos&include=teams.conference.name&include=teams.division.name&include=teams.franchiseTeam.firstSeason.id&include=teams.franchiseTeam.lastSeason.id"

    response = requests.get(url).json()

    team_df = pd.json_normalize(response['data'])

    # Add meta data (datetime of the execution)
    team_df["meta_datetime"] = pd.to_datetime("now")

    return team_df

scrapeTeams1()

"""## Playoffs"""

def scrapePlayoffs():
    url = "https://records.nhl.com/site/api/award-details?cayenneExp=trophyCategoryId=1%20and%20trophyId=1&include=seasonId&include=team.id&include=team.franchiseId&include=team.fullName&include=team.placeName&include=team.commonName&include=team.triCode&include=team.logos&include=team.active&include=team.league.abbreviation&include=coach.firstName&include=coach.lastName&include=coach.id&include=status&include=imageUrl&include=isRookie&include=summary&include=playerImageUrl&sort=seasonId&dir=DESC"

    response = requests.get(url).json()

    playoffs_df = pd.json_normalize(response['data'])

    # Add meta data (datetime of the execution)
    playoffs_df["meta_datetime"] = pd.to_datetime("now")

    return playoffs_df

scrapePlayoffs()

"""## Team-level

### Season by season record
"""

def scrapeRecords(franchiseId):

    url = f"https://records.nhl.com/site/api/franchise-season-results?cayenneExp=franchiseId={franchiseId}&include=[%22season.conferencesInUse%22,%20%22season.divisionsInUse%22]&sort=seasonId&dir=DESC"

    response = requests.get(url).json()

    records_df = pd.json_normalize(response['data'])

    # Add meta data (datetime of the execution)
    records_df["meta_datetime"] = pd.to_datetime("now")

    return records_df

scrapeRecords(1)

"""### Historical Record vs opponents"""

def scrapeRecordsOpponents(franchiseId, playoffs=False):

    if playoffs:
      url = f"https://records.nhl.com/site/api/playoff-franchise-vs-team?cayenneExp=teamFranchiseId={franchiseId}"

    else:
      url = f"https://records.nhl.com/site/api/all-time-record-vs-franchise?cayenneExp=teamFranchiseId={franchiseId}"

    response = requests.get(url).json()

    records_opponents_df = pd.json_normalize(response['data'])

    if playoffs:
      records_opponents_df["playoffs"] = True
    else:
      records_opponents_df["playoffs"] = False



    # Add meta data (datetime of the execution)
    records_opponents_df["meta_datetime"] = pd.to_datetime("now")

    return records_opponents_df

scrapeRecordsOpponents(1, playoffs=True)

"""### TeamDraft"""

def scrapeTeamDraft(franchiseId):

    url = f"https://records.nhl.com/site/api/draft?include=draftProspect.id&include=franchiseTeam&include=player.birthStateProvince&include=player.birthCountry&include=player.position&include=player.onRoster&include=player.yearsPro&include=player.firstName&include=player.lastName&include=player.id&include=team.id&include=team.placeName&include=team.commonName&include=team.fullName&include=team.triCode&include=team.logos&cayenneExp=franchiseTeam.franchiseId=%22{franchiseId}%22"

    response = requests.get(url).json()

    team_draft_df = pd.json_normalize(response['data'])
    team_draft_df["meta_datetime"] = pd.to_datetime("now")

    return team_draft_df

scrapeTeamDraft(1)

"""### Roster"""

def scrapeRoster(teamId):

    url = f"https://records.nhl.com/site/api/player/byTeam/{teamId}?include=id&include=firstName&include=lastName&include=sweaterNumber&include=position&include=height&include=weight&include=birthDate&include=birthCountry&include=birthCity&include=birthStateProvince&include=onRoster"
    response = requests.get(url).json()

    roster_df = pd.json_normalize(response['data'])

    roster_df["fullName"] = roster_df["firstName"] + " " + roster_df["lastName"]
    roster_df["teamId"] = teamId

    roster_df["playerId"] = roster_df["id"]
    roster_df["meta_datetime"] = pd.to_datetime("now")

    return roster_df

scrapeRoster(8)

"""### Retired numbers"""

def scrapeRetiredNumbers():

    # Set up Chrome options for headless browsing
    chrome_options = Options()
    chrome_options.add_argument("--headless")  # Run in headless mode
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")

    # Initialize the WebDriver
    driver = webdriver.Chrome(options=chrome_options)

    url = f"https://records.nhl.com/mtl/franchises/montreal-canadiens/history"

    driver.get(url)


    xpath = '//*[@id="mainSection"]/main/div/div[5]/div[2]/div[2]/div/div[1]/div[1]/div/ul'

    t


    # Select all element in the list and add it to list

"""## Draft"""

def scrapeDraft1(draftYear):

    """

    Scrapes draft data from the NHL website (NHL Records) for a given year. An advantage of this scraping function over the other one is that is returns the NHL playerId for all players who have played in the NHL.

    Parameters:
      - draftYear (int) : The year you want to scrape the draft data for in the format YYYY.

      Returns:
      - draft_df (pd.DataFrame) : A DataFrame containing the scraped draft data.

    """


    url = f"https://records.nhl.com/site/api/draft?include=draftProspect.id&include=player.birthStateProvince&include=player.birthCountry&include=player.position&include=player.onRoster&include=player.yearsPro&include=player.firstName&include=player.lastName&include=player.id&include=team.id&include=team.placeName&include=team.commonName&include=team.fullName&include=team.triCode&include=team.logos&include=franchiseTeam.franchise.mostRecentTeamId&include=franchiseTeam.franchise.teamCommonName&include=franchiseTeam.franchise.teamPlaceName&cayenneExp=%20draftYear%20=%20{draftYear}&start=0&limit=300"

    response = requests.get(url).json()

    draft_df = pd.json_normalize(response['data'])

    # Add meta data (datetime of the execution)
    draft_df["meta_datetime"] = pd.to_datetime("now")

    return draft_df

scrapeDraft1(2024)

"""## Tests

"""

# time the process
start_time = time.time()
mtl_schedule = scrapeSchedule("MTL", "20242025").query("gameType == 2 and gameState == 'OFF'") # Keep REG SZN Games

pbp_list = []
rosters_list = []
shifts_list = []

losers = []

game_ids = mtl_schedule.id.to_list()

for i in tqdm(game_ids):
    try:
        pbp, roster, shifts = scrape_pbp(i)
        pbp_list.append(pbp)
        rosters_list.append(roster)
        shifts_list.append(shifts)

    except:
        losers.append(i)

end_time = time.time()
execution_time = end_time - start_time
print(f"Execution time: {execution_time} seconds")



mtl_pbps = pd.concat(pbp_list, ignore_index=True).reset_index(drop=True)
mtl_rosters = pd.concat(rosters_list, ignore_index=True).reset_index(drop=True)
mtl_shifts = pd.concat(shifts_list, ignore_index=True).reset_index(drop=True)
mtl_pbps.head()

(mtl_pbps
 .query("reason == 'teammate-blocked' and eventTeam == 'MTL'")['playerName_1']
 .value_counts()
.reset_index()
.rename(columns={"playerName_1": "playerName",})
.sort_values("count", ascending=False)
.style.background_gradient(cmap='Blues'))

(mtl_pbps
 .query("reason == 'teammate-blocked' and eventTeam == 'MTL'")['playerName_2']
 .value_counts()
.reset_index()
.rename(columns={"playerName_2": "playerName",})
.sort_values("count", ascending=False)
.style.background_gradient(cmap='Reds'))